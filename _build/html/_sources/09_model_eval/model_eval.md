# Model Evaluation

To build, compare, and evaluate different models, data scientists typically divide their data into several distinct sets, each of which serves a different purpose. The training set (typically around 60% of the available data) is reserved to build different models, the validation set (typically around 20% of the data) is reserved to tune and compare those models, and the holdout set (typically around 20% of the data) is reserved to evaluate the performance of the final model. This process is described in the subsection [Partitioning Data](data_partition.html#partitioning-data). The next subsection, [Performance Metrics](performance_metrics.html#performance-metrics), describes some common metrics that data scientists use to compare and evaluate predictive models. 